{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fdbd319",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup: Add parent directory to path to import src modules\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "# Core imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Import our modules\n",
    "from src.vae import MLPVAE, ConvVAE, MultiModalVAE, vae_loss, multimodal_vae_loss\n",
    "from src.dataset import align_multimodal_data, NumpyDataset, AudioDataset, MultiModalDatasetLazy\n",
    "from src.clustering import run_all_clusterers, reduce_dimensions_2d\n",
    "from src.evaluation import compute_all_metrics, compare_methods, print_metrics_summary\n",
    "from src.visualization import plot_latent_space_2d, plot_training_curves, plot_metrics_comparison\n",
    "\n",
    "# Configuration\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "print(f\"Device: {DEVICE}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61943d31",
   "metadata": {},
   "source": [
    "## 1. Data Loading and Alignment\n",
    "\n",
    "Load and align multi-modal data (audio, lyrics, genre) across track IDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7999ee57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load aligned multi-modal data\n",
    "kept_ids, X_lyrics, X_genre, y_genre, audio_map, F, T, lyrics_dim, genre_dim = align_multimodal_data(\n",
    "    audio_dir=\"../Audio_Features\",\n",
    "    lyrics_csv=\"../Lyrics_Processed/lyrics_cleaned.csv\",\n",
    "    genre_csv=\"../genre_processed.csv\",\n",
    "    sbert_npy=\"../Lyrics_Processed/lyrics_sbert_embeddings.npy\",\n",
    "    audio_key=\"mfcc\",\n",
    "    keep_missing=True\n",
    ")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"Data Alignment Summary\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Total tracks: {len(kept_ids)}\")\n",
    "print(f\"Audio shape: (1, {F}, {T})\")\n",
    "print(f\"Lyrics dim: {lyrics_dim}\")\n",
    "print(f\"Genre classes: {genre_dim}\")\n",
    "print(f\"Tracks with genre labels: {(y_genre != -1).sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb3ab1c3",
   "metadata": {},
   "source": [
    "## 2. Easy Task: Basic MLP-VAE\n",
    "\n",
    "Train a basic MLP-VAE on audio mean features and perform K-Means clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cd1a426",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-computed features from Easy Task results\n",
    "import os\n",
    "\n",
    "if os.path.exists(\"../Results/EasyTask/Z_vae.npy\"):\n",
    "    print(\"Loading Easy Task results...\")\n",
    "    Z_easy = np.load(\"../Results/EasyTask/Z_vae.npy\")\n",
    "    results_easy = pd.read_csv(\"../Results/EasyTask/easy_task_metrics.csv\")\n",
    "    \n",
    "    print(\"\\nEasy Task Results:\")\n",
    "    print(results_easy)\n",
    "    \n",
    "    # Visualize latent space\n",
    "    from src.clustering import run_kmeans\n",
    "    labels, _ = run_kmeans(Z_easy, n_clusters=6, random_state=SEED)\n",
    "    \n",
    "    Z_2d = reduce_dimensions_2d(Z_easy[:2000], method=\"umap\", random_state=SEED)\n",
    "    plot_latent_space_2d(Z_2d, labels[:2000], title=\"Easy Task: MLP-VAE Latent Space (UMAP)\")\n",
    "else:\n",
    "    print(\"Easy Task results not found. Run the main notebook first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88caaa2b",
   "metadata": {},
   "source": [
    "## 3. Medium Task: ConvVAE with Hyperparameter Tuning\n",
    "\n",
    "ConvVAE for 2D audio features + systematic hyperparameter search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30c3b13e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Medium Task results\n",
    "if os.path.exists(\"../Results/MediumTask_WithARI/BEST_clustering_results.csv\"):\n",
    "    print(\"Loading Medium Task results...\")\n",
    "    results_medium = pd.read_csv(\"../Results/MediumTask_WithARI/BEST_clustering_results.csv\")\n",
    "    hp_comparison = pd.read_csv(\"../Results/MediumTask_WithARI/hyperparameter_comparison.csv\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"Medium Task: Hyperparameter Comparison\")\n",
    "    print(\"=\"*80)\n",
    "    print(hp_comparison)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"Medium Task: Best Configuration Results\")\n",
    "    print(\"=\"*80)\n",
    "    print(results_medium)\n",
    "    \n",
    "    # Visualize metrics comparison\n",
    "    plot_metrics_comparison(results_medium, figsize=(16, 10))\n",
    "else:\n",
    "    print(\"Medium Task results not found. Run the main notebook first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af30150b",
   "metadata": {},
   "source": [
    "## 4. Hard Task: CVAE/Beta-VAE with Complete Metrics\n",
    "\n",
    "Conditional VAE with genre conditioning + all evaluation metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98e6ad06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Hard Task results\n",
    "if os.path.exists(\"../Results/HardTask_CVAE/BEST_hard_results.csv\"):\n",
    "    print(\"Loading Hard Task results...\")\n",
    "    results_hard = pd.read_csv(\"../Results/HardTask_CVAE/BEST_hard_results.csv\")\n",
    "    cvae_comparison = pd.read_csv(\"../Results/HardTask_CVAE/cvae_comparison.csv\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"Hard Task: CVAE Configuration Comparison\")\n",
    "    print(\"=\"*80)\n",
    "    print(cvae_comparison)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"Hard Task: Best CVAE Results (All Metrics)\")\n",
    "    print(\"=\"*80)\n",
    "    print(results_hard)\n",
    "    \n",
    "    # Plot metrics comparison\n",
    "    plot_metrics_comparison(results_hard, figsize=(16, 12))\n",
    "else:\n",
    "    print(\"Hard Task results not found. Run the main notebook first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e67d423",
   "metadata": {},
   "source": [
    "## 5. Training Example: ConvVAE from Scratch\n",
    "\n",
    "Demonstrate training a ConvVAE model (small example)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c179eee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Train a small ConvVAE (demonstration only)\n",
    "DEMO_MODE = True  # Set to True for quick demo, False for full training\n",
    "\n",
    "if DEMO_MODE:\n",
    "    print(\"Demo mode: Using small subset and few epochs\")\n",
    "    sample_size = 100\n",
    "    epochs = 5\n",
    "    batch_size = 16\n",
    "else:\n",
    "    sample_size = len(kept_ids)\n",
    "    epochs = 30\n",
    "    batch_size = 32\n",
    "\n",
    "# Load audio features for a subset\n",
    "from src.dataset import load_audio_2d\n",
    "\n",
    "indices = np.random.choice(len(kept_ids), min(sample_size, len(kept_ids)), replace=False)\n",
    "X_audio_demo = []\n",
    "\n",
    "for idx in indices:\n",
    "    tid = kept_ids[idx]\n",
    "    if tid in audio_map:\n",
    "        audio = load_audio_2d(audio_map[tid], key=\"mfcc\", max_time=T)\n",
    "        X_audio_demo.append(audio[np.newaxis, :])\n",
    "    else:\n",
    "        X_audio_demo.append(np.zeros((1, F, T), dtype=np.float32))\n",
    "\n",
    "X_audio_demo = np.array(X_audio_demo)\n",
    "print(f\"Loaded {len(X_audio_demo)} audio samples: shape {X_audio_demo.shape}\")\n",
    "\n",
    "# Create model\n",
    "model = ConvVAE(in_channels=1, latent_dim=32, F=F, T=T).to(DEVICE)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "# Split data\n",
    "train_idx, val_idx = train_test_split(np.arange(len(X_audio_demo)), test_size=0.2, random_state=SEED)\n",
    "train_loader = DataLoader(AudioDataset(X_audio_demo[train_idx]), batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(AudioDataset(X_audio_demo[val_idx]), batch_size=batch_size, shuffle=False)\n",
    "\n",
    "print(f\"\\nTraining ConvVAE: {epochs} epochs, batch_size={batch_size}\")\n",
    "print(f\"Train samples: {len(train_idx)}, Val samples: {len(val_idx)}\")\n",
    "\n",
    "# Training loop\n",
    "history = []\n",
    "for epoch in range(1, epochs + 1):\n",
    "    model.train()\n",
    "    train_losses = []\n",
    "    \n",
    "    for batch in train_loader:\n",
    "        batch = batch.to(DEVICE)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        x_hat, mu, logvar, _ = model(batch)\n",
    "        loss, recon, kl = vae_loss(batch, x_hat, mu, logvar, beta=1.0)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_losses.append(loss.item())\n",
    "    \n",
    "    model.eval()\n",
    "    val_losses = []\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            batch = batch.to(DEVICE)\n",
    "            x_hat, mu, logvar, _ = model(batch)\n",
    "            loss, recon, kl = vae_loss(batch, x_hat, mu, logvar, beta=1.0)\n",
    "            val_losses.append(loss.item())\n",
    "    \n",
    "    train_loss = np.mean(train_losses)\n",
    "    val_loss = np.mean(val_losses)\n",
    "    history.append({'epoch': epoch, 'train_loss': train_loss, 'val_loss': val_loss})\n",
    "    \n",
    "    print(f\"Epoch {epoch}/{epochs} - Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")\n",
    "\n",
    "# Plot training curve\n",
    "history_df = pd.DataFrame(history)\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(history_df['epoch'], history_df['train_loss'], label='Train')\n",
    "plt.plot(history_df['epoch'], history_df['val_loss'], label='Validation')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('ConvVAE Training (Demo)')\n",
    "plt.legend()\n",
    "plt.grid(alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nTraining complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "416250a3",
   "metadata": {},
   "source": [
    "## 6. Clustering with Learned Features\n",
    "\n",
    "Extract latent features and perform clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2f444fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract latent features\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    X_tensor = torch.from_numpy(X_audio_demo).to(DEVICE)\n",
    "    _, mu, _, _ = model(X_tensor)\n",
    "    Z_demo = mu.cpu().numpy()\n",
    "\n",
    "print(f\"Extracted latent features: {Z_demo.shape}\")\n",
    "\n",
    "# Get corresponding genre labels\n",
    "y_demo = y_genre[indices]\n",
    "\n",
    "# Run clustering\n",
    "results = run_all_clusterers(\n",
    "    X=Z_demo,\n",
    "    y_true=y_demo,\n",
    "    n_clusters=6,\n",
    "    prefix=\"Demo_\",\n",
    "    compute_metrics_fn=compute_all_metrics,\n",
    "    random_state=SEED\n",
    ")\n",
    "\n",
    "# Compare methods\n",
    "comparison = compare_methods(results)\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Clustering Results Comparison\")\n",
    "print(\"=\"*80)\n",
    "print(comparison)\n",
    "\n",
    "# Visualize best clustering\n",
    "best_method, best_labels, best_metrics = results[0]\n",
    "print(f\"\\nBest method: {best_method}\")\n",
    "print_metrics_summary(best_metrics, best_method)\n",
    "\n",
    "# 2D visualization\n",
    "Z_2d_demo = reduce_dimensions_2d(Z_demo, method=\"umap\", random_state=SEED)\n",
    "plot_latent_space_2d(Z_2d_demo, best_labels, title=f\"Demo: {best_method}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed49893e",
   "metadata": {},
   "source": [
    "## 7. Results Summary\n",
    "\n",
    "Load and display all task results for comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbc5ec87",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PROJECT RESULTS SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Easy Task\n",
    "if os.path.exists(\"../Results/EasyTask/easy_task_metrics.csv\"):\n",
    "    easy_metrics = pd.read_csv(\"../Results/EasyTask/easy_task_metrics.csv\")\n",
    "    print(\"\\nüìä EASY TASK (MLP-VAE):\")\n",
    "    print(easy_metrics.to_string(index=False))\n",
    "\n",
    "# Medium Task\n",
    "if os.path.exists(\"../Results/MediumTask_WithARI/hyperparameter_comparison.csv\"):\n",
    "    medium_hp = pd.read_csv(\"../Results/MediumTask_WithARI/hyperparameter_comparison.csv\")\n",
    "    print(\"\\nüìä MEDIUM TASK (ConvVAE) - Hyperparameter Comparison:\")\n",
    "    print(medium_hp.to_string(index=False))\n",
    "    \n",
    "    best_config = medium_hp.iloc[0]\n",
    "    print(f\"\\nüèÜ Best Configuration: {best_config['Configuration']}\")\n",
    "    print(f\"   Silhouette: {best_config['Best Silhouette']:.4f}\")\n",
    "    print(f\"   Latent Dim: {best_config['Latent Dim']}, Beta: {best_config['Beta']}\")\n",
    "\n",
    "# Hard Task\n",
    "if os.path.exists(\"../Results/HardTask_CVAE/cvae_comparison.csv\"):\n",
    "    hard_cvae = pd.read_csv(\"../Results/HardTask_CVAE/cvae_comparison.csv\")\n",
    "    print(\"\\nüìä HARD TASK (CVAE/Beta-VAE) - Configuration Comparison:\")\n",
    "    print(hard_cvae.to_string(index=False))\n",
    "    \n",
    "    best_cvae = hard_cvae.iloc[0]\n",
    "    print(f\"\\nüèÜ Best CVAE Configuration: {best_cvae['Configuration']}\")\n",
    "    print(f\"   ARI: {best_cvae['Best ARI']:.4f}\")\n",
    "    print(f\"   Beta: {best_cvae['Beta']}, Latent Dim: {best_cvae['Latent Dim']}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"‚úÖ All tasks completed successfully!\")\n",
    "print(\"üìÅ Results saved in: Results/EasyTask, Results/MediumTask_WithARI, Results/HardTask_CVAE\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dcc3eda",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This notebook demonstrates:\n",
    "1. ‚úÖ **Easy Task**: Basic MLP-VAE with K-Means clustering\n",
    "2. ‚úÖ **Medium Task**: ConvVAE with systematic hyperparameter tuning and ARI computation\n",
    "3. ‚úÖ **Hard Task**: CVAE/Beta-VAE with complete metrics (Silhouette, ARI, NMI, Purity)\n",
    "4. ‚úÖ **All metrics**: 6 evaluation metrics implemented and compared\n",
    "5. ‚úÖ **Visualizations**: Latent space, training curves, cluster distributions\n",
    "\n",
    "### Key Achievements:\n",
    "- Multi-modal VAE for audio + lyrics + genre\n",
    "- Systematic hyperparameter search\n",
    "- Comprehensive baseline comparisons\n",
    "- Production-quality modular code\n",
    "\n",
    "### Next Steps:\n",
    "1. Write NeurIPS-style paper using results from `Results/` directories\n",
    "2. Upload repository to GitHub\n",
    "3. Optional: Further hyperparameter tuning or architecture experiments"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
